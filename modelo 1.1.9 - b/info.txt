dias = [0,1,2,3,4,5,6,7]  # Lunes, Miércoles, Viernes
            horas = [1,2,3,4,5,6,7,8,9, 10, 11, 12,13,14,15,16,17,18,19,20,21,22,23]  # De 9 a 12 horas
            df = cargar_datos_especificos('potencias.csv', 'corrientes.csv', dias_semanales=dias, horas=horas)
            #df = codificar_tiempo(df)
            #X= crear_ventana_dataset(df,4)
            print(df.shape)
            X, y = crear_ventana(df[40000:120000], 12*4,4*6)
            
            inicio_train = 0
            fin_train = 45000
            inicio_val = fin_train+1 
            fin_val = fin_train+1+15000
            inicio_test = fin_val+1
            fin_test = inicio_test+1+20000
            # conjunto de validación
            Xval = X[inicio_val:fin_val]
            yval = y[inicio_val:fin_val]
            #conjunto de entrenamiento
            Xtrain = X[inicio_train:fin_train]
            ytrain = y[inicio_train:fin_train]
            # conjunto de validación
            Xtest = X[inicio_test:fin_test]
            ytest = y[inicio_test:fin_test]

            from sklearn.preprocessing import MinMaxScaler
            scaleractiva = MinMaxScaler(feature_range=(0, 1))
            Xtrain_n = Xtrain.copy()
            Xtrain_n[:, :, 0] = scaleractiva.fit_transform(Xtrain[:, :, 0])
            Xval_n = Xval.copy()
            Xval_n[:, :, 0] = scaleractiva.transform(Xval[:, :, 0])
            Xtest_n = Xtest.copy()
            Xtest_n[:, :, 0] = scaleractiva.transform(Xtest[:, :, 0])
            print(Xtest_n)
            print("la dimension es ", Xtest_n.shape)

            salidas = MinMaxScaler(feature_range=(0, 1))
            ytrain_n = ytrain.copy()
            ytrain_n = salidas.fit_transform(ytrain)
            yval_n = yval.copy()
            yval_n = salidas.transform(yval)
            ytest_n = salidas.transform(ytest)

            
            import pickle
            scalers = {'scaleractiva': scaleractiva, 'salidas': salidas}

            with open('scalers.pkl', 'wb') as f:
                pickle.dump(scalers, f)

            print("media ", scaleractiva.data_min_)
            print("desv ", scaleractiva.data_max_)
            #X_n = escalar_entrada(X,scal)
            from sklearn.utils import shuffle

            #Xtrain_n, ytrain_n = shuffle(Xtrain_n, ytrain_n, random_state=0)
            #Xval_n, yval_n = shuffle(Xval_n, yval_n, random_state=0)



modelo:

def entrenar_modelo(Xtrain, ytrain, Xval, yval, path_guardado='modelo_entrenado.h5'):
    # Asegurar reproducibilidad
    np.random.seed(47)
    tf.random.set_seed(47)
    initializer = GlorotUniform(seed=47)

    # Define los intervalos y los valores de learning rate
    boundaries = [1, 2, 20, 30, 40, 250]  # Los límites de los intervalos (épocas en este caso)
    values = [0.005, 0.002, 0.001, 0.0001, 0.00001, 0.00005, 0.000001]  # Learning rates correspondientes a los intervalos

    # Crea el scheduler de learning rate
    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
        boundaries=boundaries,
        values=values
    )
    # Crear el modelo LSTM
    model = Sequential()

    model.add(LSTM(256, return_sequences=True, input_shape=(Xtrain.shape[1], Xtrain.shape[2]), kernel_initializer=initializer ) )
    #model.add(Dropout(0.1))
    model.add(BatchNormalization())

    #model.add(Dense(128, activation='relu', kernel_initializer=initializer))
    #model.add(Dropout(0.2)) 
    model.add(LSTM(128, return_sequences=True, kernel_initializer=initializer ) )
    #model.add(Dropout(0.1))
    model.add(BatchNormalization())
    model.add(LSTM(64, return_sequences=False, kernel_initializer=initializer ) )
    #model.add(Dropout(0.1))
    model.add(BatchNormalization())

    model.add(Dense(ytrain.shape[1], activation="sigmoid"))

    # Compilar el modelo con el optimizador personalizado
    optimizer = Adam(learning_rate=lr_schedule, clipnorm=1)
    model.compile(optimizer=optimizer, loss='huber_loss')

    # EarlyStopping para evitar sobreajuste
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # ModelCheckpoint para guardar el modelo durante el entrenamiento
    checkpoint = ModelCheckpoint(path_guardado, monitor='val_loss', save_best_only=True, verbose=1)

    try:
        # Entrenar el modelo con datos de validación, EarlyStopping y ModelCheckpoint
        model.fit(Xtrain, ytrain, epochs=250, verbose=1, batch_size=16,
                  validation_data=(Xval, yval), callbacks=[early_stopping, checkpoint])
    except MemoryError as e:
        print("Error de memoria: ", e)
        print("Guardando el modelo hasta el último punto alcanzado...")
        model.save(path_guardado)  # Guarda el modelo al momento de fallo
    except Exception as e:
        print(f"Se produjo un error: {e}")
        print("Guardando el modelo hasta el último punto alcanzado...")
        model.save(path_guardado)  # Guarda el modelo si ocurre otro error

    return model
